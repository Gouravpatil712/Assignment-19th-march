{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df406c88-40af-4ffa-ad91-8c158049146d",
   "metadata": {},
   "source": [
    "## Question 01 - What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8deeca-3737-4477-b33e-70d55b98fe0a",
   "metadata": {},
   "source": [
    "## Answer :-\n",
    "\n",
    "Missing values are the values that are not present in a dataset. They can occur due to various reasons such as data corruption, human errors during data entry, data storage problems, etc. Missing values are denoted by symbols like NA, NaN, or simply left blank.\n",
    "\n",
    "Handling missing values is essential because they can lead to biased and inaccurate models. If the number of missing values is significant, it can also affect the statistical power of the analysis. Hence, it is crucial to identify the missing values and handle them appropriately.\n",
    "\n",
    "Some algorithms that are not affected by missing values are:\n",
    "\n",
    "1. Decision Trees: Decision Trees can handle missing values by ignoring the missing values and creating split based on the available data.\n",
    "\n",
    "2. Random Forest: Random Forest is an extension of Decision Trees that can handle missing values by ignoring the missing values while selecting the best split.\n",
    "\n",
    "3. Naive Bayes: Naive Bayes is a probabilistic algorithm that can handle missing values by ignoring them.\n",
    "\n",
    "4. K-Nearest Neighbors: KNN algorithm can handle missing values by ignoring the missing values while computing the distance between data points.\n",
    "\n",
    "5. Support Vector Machines: SVM can handle missing values by ignoring them while computing the distance between data points.\n",
    "\n",
    "However, some algorithms are sensitive to missing values, such as linear regression, logistic regression, k-means clustering, etc. Hence, it is crucial to handle missing values before applying these algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8aa6126-da0f-417c-96e6-4ae6224f59ce",
   "metadata": {},
   "source": [
    "## Question 02 - List down techniques used to handle missing data. Give an example of each with python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c74c64-1fa5-42de-89cd-194eacb47542",
   "metadata": {},
   "source": [
    "## Answer :-\n",
    "\n",
    "There are several techniques to handle missing data in a dataset. Some of the common techniques are:\n",
    "\n",
    "Deletion: Delete the rows or columns containing missing data.\n",
    "Imputation: Fill in the missing values with estimated values.\n",
    "Here are examples of how to implement each technique using Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "331c9b9f-fe19-4e9f-affe-73e5127c9a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deletion:\n",
    "\n",
    "#a. Listwise Deletion: Delete all rows that contain missing data.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# create a sample dataframe with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4], 'B': [5, np.nan, np.nan, 8], 'C': [9, 10, 11, 12]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddddd94b-6c17-4e4a-b18a-00643eeed8a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A    B   C\n",
       "0  1.0  5.0   9\n",
       "1  2.0  NaN  10\n",
       "2  NaN  NaN  11\n",
       "3  4.0  8.0  12"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "287850f6-30db-40e3-a264-fb3e290f0c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B   C\n",
      "0  1.0  5.0   9\n",
      "3  4.0  8.0  12\n"
     ]
    }
   ],
   "source": [
    "# drop rows with missing values\n",
    "df_new = df.dropna()\n",
    "\n",
    "# print the new dataframe\n",
    "print(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96efafec-0188-4674-b72b-5f7477d37221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A    B   C\n",
       "0  1.0  5.0   9\n",
       "1  2.0  NaN  10\n",
       "2  NaN  NaN  11\n",
       "3  4.0  8.0  12"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b. Pairwise Deletion: Delete only the rows or columns that have missing values for specific features.\n",
    "\n",
    "# create a sample dataframe with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4], 'B': [5, np.nan, np.nan, 8], 'C': [9, 10, 11, 12]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8743326-a81a-438e-b996-5fb4e119f041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B   C\n",
      "0  1.0  5.0   9\n",
      "3  4.0  8.0  12\n"
     ]
    }
   ],
   "source": [
    "# drop rows with missing values for column 'B'\n",
    "df_new = df.dropna(subset=['B'])\n",
    "\n",
    "# print the new dataframe\n",
    "print(df_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99ea6e61-9b0e-49c7-99f7-92e7ad03bc08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A    B   C\n",
       "0  1.0  5.0   9\n",
       "1  2.0  NaN  10\n",
       "2  NaN  NaN  11\n",
       "3  4.0  8.0  12"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imputation:\n",
    "\n",
    "# a. Mean Imputation: Replace missing values with the mean of the corresponding feature.\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# create a sample dataframe with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4], 'B': [5, np.nan, np.nan, 8], 'C': [9, 10, 11, 12]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0be900d-5b34-43b5-8d4d-4af238ec6065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          A    B     C\n",
      "0  1.000000  5.0   9.0\n",
      "1  2.000000  6.5  10.0\n",
      "2  2.333333  6.5  11.0\n",
      "3  4.000000  8.0  12.0\n"
     ]
    }
   ],
   "source": [
    "# create an imputer object with mean strategy\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# fit the imputer on the dataframe\n",
    "imputer.fit(df)\n",
    "\n",
    "# transform the dataframe by replacing missing values with mean\n",
    "df_new = pd.DataFrame(imputer.transform(df), columns=df.columns)\n",
    "\n",
    "# print the new dataframe\n",
    "print(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f29d8ba-460d-4bcb-a3c0-745e2d10a5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B     C\n",
      "0  1.0  5.0   9.0\n",
      "1  2.0  6.5  10.0\n",
      "2  2.0  6.5  11.0\n",
      "3  4.0  8.0  12.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# create a sample dataframe with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4], 'B': [5, np.nan, np.nan, 8], 'C': [9, 10, 11, 12]})\n",
    "\n",
    "# create an imputer object with median strategy\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# fit the imputer on the dataframe\n",
    "imputer.fit(df)\n",
    "\n",
    "# transform the dataframe by replacing missing values with median\n",
    "df_new = pd.DataFrame(imputer.transform(df), columns=df.columns)\n",
    "print(df_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6d41d6-13b0-4c2e-9125-316f731926ce",
   "metadata": {},
   "source": [
    "## Question 03 - Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566aea2a-e816-4990-9e37-5dada704cfec",
   "metadata": {},
   "source": [
    "## Answer :-\n",
    "\n",
    "Imbalanced data refers to a situation in which the classes in the target variable are not represented equally in a dataset. This means that one class may be significantly more frequent than the other(s). For example, in a binary classification problem where the positive class (class of interest) has a much smaller number of observations than the negative class.\n",
    "\n",
    "If imbalanced data is not handled properly, it can lead to biased models that have a higher accuracy in predicting the majority class and a lower accuracy in predicting the minority class. This can be particularly problematic in applications where it is more important to correctly identify the minority class, such as detecting fraudulent transactions or medical diagnoses.\n",
    "\n",
    "For example, if a dataset has 95% negative class and 5% positive class, a model that predicts all samples as negative will have an accuracy of 95%, which looks impressive but useless for detecting positive samples. On the other hand, if the model is biased towards the minority class, it can lead to high false positives, i.e., predicting a negative class as positive, which can also be detrimental in certain applications.\n",
    "\n",
    "Therefore, it is crucial to handle imbalanced data to ensure that the model is trained on a balanced dataset and that it has equal representation of all classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db174ca-547d-427c-a0ad-72703b7ebe7c",
   "metadata": {},
   "source": [
    "## Question 04 - What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down- sampling are required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe8d3f8-394a-4c57-8883-1d406a17a1f9",
   "metadata": {},
   "source": [
    "## Answer :-\n",
    "\n",
    "Up-sampling and down-sampling are techniques used in dealing with imbalanced datasets, where one class is significantly underrepresented compared to the other class.\n",
    "\n",
    "- Up-sampling: Up-sampling involves increasing the number of instances in the minority class to match the number of instances in the majority class. This can be achieved by randomly duplicating the existing instances in the minority class or by generating new synthetic samples.\n",
    "\n",
    "- Down-sampling: Down-sampling involves decreasing the number of instances in the majority class to match the number of instances in the minority class. This can be achieved by randomly selecting a subset of the instances in the majority class.\n",
    "\n",
    "An example where up-sampling might be required is in the case of fraud detection, where the number of fraudulent transactions is much less than the number of non-fraudulent transactions. In this case, up-sampling the minority class (fraudulent transactions) can help the model learn more about the rare class and improve its performance.\n",
    "\n",
    "An example where down-sampling might be required is in the case of a medical dataset where the number of healthy patients significantly exceeds the number of patients with a disease. In this case, down-sampling the majority class (healthy patients) can help to balance the dataset and prevent the model from being biased towards the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01760b5e-f784-4566-b1e1-3176f3503d25",
   "metadata": {},
   "source": [
    "## Question 05 - What is data Augmentation? Explain SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce6779f-bb4a-47ba-beac-a6b48593a266",
   "metadata": {},
   "source": [
    "## Answer :-\n",
    "\n",
    "Data augmentation is a technique in machine learning that involves creating new training data from existing data to increase the size and diversity of the training set. This helps in improving the performance and generalization of the model. SMOTE (Synthetic Minority Over-sampling Technique) is a popular data augmentation method used for handling imbalanced datasets.\n",
    "\n",
    "SMOTE works by creating synthetic samples of the minority class by selecting random samples from the minority class and creating similar but slightly different samples. SMOTE uses a k-nearest neighbor algorithm to generate these new samples. It selects a sample from the minority class and identifies its k-nearest neighbors. SMOTE then randomly selects one of these neighbors and creates a new sample that is a combination of the original sample and the selected neighbor.\n",
    "\n",
    "For example, suppose we have a dataset with two classes, A and B, where class A is the minority class. If the dataset is imbalanced, with only a few samples in class A, we can use SMOTE to generate new samples for class A. SMOTE selects a sample from class A and identifies its k-nearest neighbors. It then randomly selects one of these neighbors and creates a new sample that is a combination of the original sample and the selected neighbor. The new sample is added to the dataset as a new data point for class A.\n",
    "\n",
    "In Python, we can use the imblearn library to implement SMOTE. Here is an example code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "306f43cd-119e-4c92-918a-b34799d31ead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from imblearn.over_sampling import SMOTE\\n\\n# Load the imbalanced dataset\\nX, y = load_data()\\n\\n# Instantiate the SMOTE object\\nsm = SMOTE()\\n\\n# Fit the SMOTE object to the dataset and generate new samples\\nX_resampled, y_resampled = sm.fit_resample(X, y)'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load the imbalanced dataset\n",
    "X, y = load_data()\n",
    "\n",
    "# Instantiate the SMOTE object\n",
    "sm = SMOTE()\n",
    "\n",
    "# Fit the SMOTE object to the dataset and generate new samples\n",
    "X_resampled, y_resampled = sm.fit_resample(X, y)'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9342cc45-755b-4ee5-8e06-05ba831bbcf5",
   "metadata": {},
   "source": [
    "## Question 06 - What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2d7577-ea4f-4fd2-914a-3bf728902e0f",
   "metadata": {},
   "source": [
    "## Answer :-\n",
    "\n",
    "Outliers are data points that lie far away from the majority of the data points in a dataset. They can be caused by various reasons, such as measurement errors, data entry errors, or natural variations in the data.\n",
    "\n",
    "It is essential to handle outliers in a dataset because they can have a significant impact on the statistical analysis and machine learning algorithms. Outliers can skew the distribution of the data, affect the mean and variance, and reduce the accuracy of the model. Additionally, some machine learning algorithms are sensitive to outliers, which can result in poor performance.\n",
    "\n",
    "Handling outliers involves identifying them and then deciding whether to remove or transform them.\n",
    "\n",
    "There are various techniques to handle outliers, such as:\n",
    "\n",
    "- Trimming: removing extreme values from the dataset\n",
    "- Winsorizing: replacing extreme values with a less extreme value\n",
    "- Transforming: applying a mathematical transformation to the data, such as a logarithmic or square root transformation\n",
    "- Robust statistics: using statistical methods that are less sensitive to outliers, such as the median instead of the mean\n",
    "\n",
    "In some cases, it may be appropriate to keep the outliers in the dataset, particularly if they are a result of genuine data variability or measurement error. However, it is crucial to carefully consider the impact of outliers on the analysis and machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f2329c-f191-4065-8cff-d0f5df0d4fb1",
   "metadata": {},
   "source": [
    "## Question 07 - You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some techniques you can use to handle the missing data in your analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0597440-c0f9-45ea-9bc7-97fb4033f2d6",
   "metadata": {},
   "source": [
    "## Answer :-\n",
    "\n",
    "There are several techniques to handle missing data in a dataset. Some of the most commonly used techniques are:\n",
    "\n",
    "1. Deletion: In this method, the rows or columns containing missing values are removed from the dataset. However, this method should be used only when the percentage of missing values is very low, and the missing values are missing completely at random (MCAR).\n",
    "\n",
    "2. Mean/ Median/ Mode Imputation: In this method, the missing values are replaced with the mean, median, or mode value of the corresponding feature. This method is suitable when the missing values are missing at random (MAR).\n",
    "\n",
    "3. Forward/ Backward Fill: In this method, the missing values are replaced with the last known value (forward fill) or the next known value (backward fill). This method is suitable when the missing values occur in a sequence and are missing at random (MAR).\n",
    "\n",
    "4. Hot Deck Imputation: In this method, the missing values are replaced with a randomly selected value from a similar group of individuals. This method is suitable when the missing values are missing not at random (MNAR) and depend on some other variables.\n",
    "\n",
    "5. Machine Learning Methods: In this method, the missing values are predicted using machine learning algorithms such as k-Nearest Neighbors (k-NN), Decision Trees, Random Forests, etc. This method is suitable when the missing values are missing not at random (MNAR) and depend on some other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c14bcbc-ca35-45ba-8581-4e0c2346263f",
   "metadata": {},
   "source": [
    "## Question 08 - You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d78117-c5cf-4c6d-9684-ab7a0bc56ef6",
   "metadata": {},
   "source": [
    "## Answer :-\n",
    "\n",
    "There are several strategies to determine if the missing data is missing at random or if there is a pattern to the missing data:\n",
    "\n",
    "1. Visual inspection: One simple way to determine if there is a pattern to the missing data is to visualize the missing data using a heatmap. This will allow you to see if the missing data is concentrated in specific areas or if it is randomly distributed across the dataset.\n",
    "\n",
    "2. Statistical tests: There are several statistical tests that can be used to determine if the missing data is missing at random or not. These tests include the Little’s MCAR test, which tests if the missing data is completely at random, and the Missing Indicator method, which tests if there is a pattern to the missing data.\n",
    "\n",
    "3. Imputation methods: Another way to determine if there is a pattern to the missing data is to use imputation methods such as mean imputation or regression imputation. If the imputed values are significantly different from the observed values, it suggests that there may be a pattern to the missing data.\n",
    "\n",
    "4. Domain knowledge: It is also essential to have domain knowledge about the dataset to determine if there is a pattern to the missing data. For example, if you are working with medical data and notice that there is a higher percentage of missing data for certain age groups, it may suggest that there is a pattern to the missing data.\n",
    "\n",
    "By using these strategies, you can determine if the missing data is missing at random or if there is a pattern to the missing data, which will help you decide on the best approach to handle the missing data in your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd4d812-3983-4ec9-bdf6-b3b24f5015e3",
   "metadata": {},
   "source": [
    "## Question 09 - Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddf4c23-67d5-404b-96eb-223db3409c8c",
   "metadata": {},
   "source": [
    "## Answer :-\n",
    "\n",
    "When dealing with an imbalanced dataset in a medical diagnosis project, the following strategies can be used to evaluate the performance of a machine learning model:\n",
    "\n",
    "1. Confusion Matrix: The confusion matrix provides a summary of the predicted results versus the actual results. It can be used to calculate metrics such as precision, recall, and F1 score, which are commonly used to evaluate the performance of classification models.\n",
    "\n",
    "2. ROC Curve and AUC: The ROC curve is a graphical representation of the true positive rate (sensitivity) against the false positive rate (1-specificity) of a classification model for different classification thresholds. The area under the ROC curve (AUC) is a performance metric that provides a single number representing the overall quality of the classification model. A model with an AUC of 1.0 indicates perfect performance, while a model with an AUC of 0.5 indicates random guessing.\n",
    "\n",
    "3. Stratified Sampling: Stratified sampling is a technique used to ensure that the imbalanced class distribution is preserved in both the training and testing sets. In this technique, the data is divided into several strata based on the class distribution, and then samples are drawn from each stratum in proportion to its size.\n",
    "\n",
    "4. Resampling Techniques: Resampling techniques are used to balance the class distribution by oversampling the minority class or undersampling the majority class. Techniques such as SMOTE (Synthetic Minority Over-sampling Technique) and ADASYN (Adaptive Synthetic Sampling) are commonly used to oversample the minority class.\n",
    "\n",
    "5. Cost-Sensitive Learning: Cost-sensitive learning involves adjusting the misclassification cost of the minority class to reduce the impact of misclassifying the minority class. This technique can be useful in situations where the cost of false negatives (missed diagnoses) is higher than the cost of false positives (incorrect diagnoses).\n",
    "\n",
    "6. Ensemble Methods: Ensemble methods such as bagging, boosting, and stacking can be used to improve the performance of a classification model on imbalanced datasets. These methods combine the predictions of multiple models to reduce the impact of individual model weaknesses and improve the overall performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f6d94d-078b-4680-85c9-28bdb94d67c1",
   "metadata": {},
   "source": [
    "## Question 10 - When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dd1e43-240a-4ede-9d69-9697e3b6bc17",
   "metadata": {},
   "source": [
    "## Answer :-\n",
    "\n",
    "To down-sample the majority class in an imbalanced dataset, we can use various techniques, including:\n",
    "\n",
    "1. Random under-sampling: This involves randomly removing samples from the majority class until the class distribution is balanced. This can be done using the resample function from the sklearn.utils module in Python. Here is an example:\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Down-sample majority class\n",
    "df_majority = df[df.Satisfaction == 'Satisfied']\n",
    "df_minority = df[df.Satisfaction == 'Not Satisfied']\n",
    "df_majority_downsampled = resample(df_majority,\n",
    "                                   replace=False,  # sample without replacement\n",
    "                                   n_samples=len(df_minority),  # match minority class\n",
    "                                   random_state=42)  # reproducible results\n",
    "\n",
    "# Combine minority class with down-sampled majority class\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "2. Cluster-based under-sampling: This involves identifying clusters of samples in the majority class and removing samples from each cluster until the class distribution is balanced. This can be done using the KMeans algorithm from the sklearn.cluster module in Python.\n",
    "\n",
    "3. Tomek links: This involves identifying pairs of samples from different classes that are closest to each other and removing the majority class samples from the pairs. This can be done using the TomekLinks class from the imblearn.under_sampling module in Python.\n",
    "\n",
    "4. Neighbourhood cleaning rule: This involves removing samples from the majority class that are classified incorrectly by a k-nearest neighbour algorithm. This can be done using the NeighbourhoodCleaningRule class from the imblearn.under_sampling module in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed897596-2a7a-47e9-abdb-aad82a5c8335",
   "metadata": {},
   "source": [
    "## Question 11 - You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd743ce2-934f-4768-809e-d4dd6ba8e378",
   "metadata": {},
   "source": [
    "## Answer :-\n",
    "\n",
    "In order to balance the dataset and up-sample the minority class, we can use various techniques. Some of the techniques are:\n",
    "\n",
    "- Random over-sampling: Randomly selecting instances from the minority class and duplicating them until the classes are balanced.\n",
    "\n",
    "- Synthetic Minority Over-sampling Technique (SMOTE): Generating synthetic samples of the minority class by interpolating between the minority class instances.\n",
    "\n",
    "- Adaptive Synthetic (ADASYN): A variant of SMOTE, which generates synthetic samples of the minority class with a greater degree of difficulty for the classifier.\n",
    "\n",
    "Here is an example of how to use the SMOTE technique for up-sampling the minority class in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f91fdc2-425d-456b-b9cc-834ffea77693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from imblearn.over_sampling import SMOTE\\n\\nX_train_resampled, y_train_resampled = SMOTE().fit_resample(X_train, y_train)'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from imblearn.over_sampling import SMOTE\n",
    "\n",
    "X_train_resampled, y_train_resampled = SMOTE().fit_resample(X_train, y_train)'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7016b8d-c15b-4b40-816c-6459298d5858",
   "metadata": {},
   "source": [
    "In the above code, X_train and y_train are the feature and target variables of the training dataset, respectively. The fit_resample() method from the SMOTE class generates synthetic samples of the minority class to balance the dataset, and returns the up-sampled feature and target variables, X_train_resampled and y_train_resampled, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02a1297-1323-40f8-b28c-1690f722afd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
